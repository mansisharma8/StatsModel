---
title: 'Statistical Models : Homework 4'
output:
  pdf_document: default
  html_document: default
author: "Mansi Sharma"
  
date: "2023-11-07"
---

# Question 1

In this problem, you practice working with predictor variables that are discrete. Consider the Boston dataset in the package MASS. Take as response the median property value.

```{r}
library(MASS)
library(ggplot2)
library(quantreg)
head(Boston)
```

### 1(a) Analysis by Charles River Proximity

We explore the variation in median values of properties based on their proximity to the Charles River, delineated by the 'chas' variable.

### Constructing Boxplot

```{r}
#Generating the box plot
boxplot(Boston$medv ~ Boston$chas, col='steelblue', 
        main = "Median Property Value(Medv) grouped by Charles River(chas)", 
        xlab = "Charles River Presence", ylab = "Median Property Value")
```

**Inference :**

-   The range of medv for properties close to the river (chas = 1) is tightly packed between the interquartile range compared to those farther away.

-   The median of medv for both categories of chas is somewhat similar, hovering around the 20-25 mark.

-   There seems to be a tendency for property values to be higher closer to the river, implying a positive influence of the river's proximity on property values.

```{r}
#Comparison of different groups
table(Boston$chas)
plot(Boston$medv,Boston$chas,pch = 19,  col=as.factor(Boston$chas))
```

Inference:

-   It's evident that properties not adjacent to the river (chas = 0) are more prevalent.

-   The spread of medv values for chas = 0 is broader, as also reflected in the boxplot.

### Model Construction

```{r}
#Fitting  a linear model
model_chas = lm(medv ~ chas, data = Boston)
summary(model_chas)
```

### Analysis of Variance (ANOVA) Examination

```{r}
anova(model_chas)$F
```

```{r}
anova(model_chas)
```

Inference :

-   The F-test assesses the variance among group means and the variance within groups.

-   An F-test value far from 1 indicates significant variance among groups.

-   Here, the F value of 15.97 indicates a notable difference in variance.

-   Null hypothesis for this case is : that the coefficients for the **`chas`** variable are equal to zero. This means that the **`chas`** variable has no effect on the median property value (**`medv`**). The null hypothesis assumes that the **`chas`** variable is not a significant predictor of **`medv`**. The alternative hypothesis is that the coefficients for the **`chas`** variable are not equal to zero, meaning that the **`chas`** variable does have an effect on **`medv`** and is a significant predictor.

-   Since the p - value if smaler than the significance level (0.05), the null hypothesis fails and we conclude that the **`chas`** variable is significantly related to **`medv`** meaning that the **`chas`** variable does have an effect on **`medv`** and is a significant predictor.

-   This result is also consistent with what we can see from the boxplots that Charles river has impact that proximity to the Charles River has a positive effect on median property values.

### 1(b) Analysis by Accessibility to Highways

We now examine how the variable rad, representing highway accessibility, affects median property values.

### Generating boxplot

```{r}
#Creating boxplot 
boxplot(Boston$medv ~ Boston$rad, col='steelblue', 
        main = "Median Property Value(Medv) grouped by rad", 
        xlab = "rad", ylab = "Median Property Value")
```

Inference:

-   Since, the boxes are of different heights, we can say that the median property value is not the same across all levels of **`rad.`**

-   We can also observe that for higher value of rad (24) , the median is less as compared to the closer rad values(1-8), that is for higher rad value (24 ) we can see that medv values is less. This means that the accessibility to radial highways generally lowers the median property values.

```{r}
#Visually seeing the difference
table(Boston$rad)
plot(Boston$medv,Boston$rad,pch = 19,  col=as.factor(Boston$rad))
```

Inference:

-   We can see that the data values for rad from 1-8 are quite well spread but for rad =24 are sparsely spread out with one data point above 40.

### Fitting the model

```{r}
#Fitting the model
model_rad = lm(medv ~ rad, data = Boston)
summary(model_rad)
```

### Analysis of Variance (ANOVA)

```{r}
anova(model_rad)$F
```

```{r}
anova(model_rad)
```

Inference : What is the F-test testing? Is the result consistent with the boxplots?

-   F-test testing procedure that compares the variability of two or more sets of data

-   For F test value =1 the variances are equal which is Null hypothesis and if it is not equal to 1 then there is variability

-   We can also see that the F value here is 85.91 which denotes significant variability

-   Null hypothesis for this case is : that the coefficients for the rad variable are equal to zero. This means that the rad variable has no effect on the median property value (**`medv`**). The null hypothesis assumes that the rad variable is not a significant predictor of **`medv`**. The alternative hypothesis is that the coefficients for the rad variable are not equal to zero, meaning that the **`rad`** variable does have an effect on **`medv`** and is a significant predictor.

-   Since the p - value if smaller than the significance level (0.05), the null hypothesis fails and we conclude that the **`rad`** variable is significantly related to **`medv`** meaning that the **`rad`** variable does have an effect on **`medv`** and is a significant predictor.

-   This result is also consistent with what we can see from the boxplots that less accessibility to highways has an effect on median property values

### 1(c) Boxplot

Now we investigate the joint effect of proximity to the Charles River (`chas`) and highway accessibility (`rad`) on median property values.


```{r}
boxplot(medv ~ chas * rad, data = Boston, 
        main = "Median Property Value by Charles River and Accessibility Index", 
        xlab = "Charles River and Accessibility Index", 
        ylab = "Median Property Value",col=c("purple", "pink"))

legend("bottom", inset = 0.02, title = "Legend", c("chas : 0", "chas :1"),
       fill = c("purple", "pink"), horiz = TRUE, cex = 0.6)

```

Inference:

-   We can see that the grouping is done in 2 forms for chas where 1 and 0 indicate pink and purple boxes along with 9 values of rad for each, hence it creates 18 plots

-   Another observation is that for some chas groups values(1) the rad values are missing.

### Interaction plot

#### Grouped by Chas

```{r fig.align='center'}
interaction.plot(Boston$rad, Boston$chas, Boston$medv, col = Boston$medv)
```

Inference: We can see the variable trend for different values of rad for each of 0 and 1 groups chas.

#### Grouped by Rad

```{r fig.align='center'}
interaction.plot(Boston$chas, Boston$rad, Boston$medv,
                 xlab = "Charles River", 
                 ylab = "Accessibility Index", col = Boston$medv)
```

Inference: We can see the lines for rad of different groups of chas for each of 0 and 1 groups

### Fitting model and ANOVA

```{r}
model_combined= lm(medv ~ chas * rad, data = Boston)
summary(model_combined)
anova(model_combined)
```

Inference:

F tests are used for different purposes that is for:

-   testing equality of variance to test hypothesis of equality of two population variances

-   testing equality of several means to test for equality of several means is carried out by the technique called ANOVA

-   for testing significance of regression is used to test the significance of the regression model

Comparision to previous F test:

As seen before, the chas and rad F test values were 15.97 and 85.91 but here we can see an increase in F test values to be 19.10 and 89.97 for degree of freedom 1

We can also see the combined effect of chas and rad where the F test values is 10.86 for degree of freedom 1. We can also see a decrease in significance of p values.

The observed values don't seem very consistent with the plots

### 1(d) Investigating the influence of lower status population

We explore whether the impact of the lower status population (lstat) on median property values (medv) is modified by proximity to the Charles River (chas).

```{r}
plot(Boston$medv ~ Boston$lstat, col='steelblue', 
     main = "Median Property Value(Medv) vs  Lstat", 
     xlab = "Lstat", ylab = "Median Property Value")
m2 = lm(Boston$medv ~ Boston$lstat)
abline(m2, col="black")
```

We can observe that as the the value of lstat decreases, the values of medv increases. It is almost inversely proportional.

```{r}
#Producing a plot to show our results
ind = (Boston$chas==0)
plot(Boston$medv[ind] ~ Boston$lstat[ind], data = Boston, 
     main = "Plot medv vs lstat",col="pink",
     xlab="Lstat", ylab = "Medv")
fit_1 = lm(Boston$medv[ind] ~ Boston$lstat[ind], data = Boston)
abline(fit_1, col="pink", lwd=2)  
ind = (Boston$chas==1)
points(Boston$medv[ind] ~ Boston$lstat[ind], data = Boston, col="purple", pch=19)
fit_2 = lm(Boston$medv[ind] ~ Boston$lstat[ind], data = Boston)
abline(fit_2, col="purple", lwd=2)  
```

```{r}
anova(fit_1)
```

```{r}
anova(fit_2)
```

Inference:

-   We can see from the interaction plot that the houses near the Charles river (chas =1 ) that is the purple dots has a higher median property value as compared to the ones that are far away from the Charles river(chas =0) that is the pink dots.

-   To test the hypothesis that the rate of decrease of median property value depends on whether the area borders the Charles River, we can perform hypothesis testing. First, we need to formulate our null and alternative hypotheses.

-   Our null hypothesis might be that the rate of decrease in median property value with increasing lstat is the same for neighborhoods that border the Charles River as it is for neighborhoods that do not. Our alternative hypothesis might be that the rate of decrease is different for these two groups.

-   As we can see that both are highly significant in the hypothesis testing but the variability in both the F values is significantly different. Thus this disregards the null hypothesis and satisfies our alternate hypothesis.

# Question 2

Transitioning to the problem of fitting a polynomial model to predict medv as a function of lstat.

### Fit a polynomial model of degree 1

First we will fit a model of degree of freedom = 1

```{r}
#Fit a polynomial model of degree 1 
fit <- lm(medv ~ poly(lstat, 1), data = Boston)
plot(Boston$lstat, Boston$medv, 
     main = "Median Property Value vs. Lower Status Population(DEGREE 1)", 
     xlab = "Lower Status Population", 
     ylab = "Median Property Value")
lines(sort(Boston$lstat), predict(fit, 
      newdata = data.frame(lstat = sort(Boston$lstat))), 
      col = "purple")
#summary(fit)
```

**2a** Fit a polynomial model of degree 3 by least squares.

### Fit a polynomial model of degree 3 by least squares.

```{r}
#Fit a polynomial model of degree 3 by least squares.
fit <- lm(medv ~ poly(lstat, 3), data = Boston)
plot(Boston$lstat, Boston$medv, 
     main = "Median Property Value vs. Lower Status Population (DEGREE 3)", 
     xlab = "Lower Status Population", 
     ylab = "Median Property Value")
lines(sort(Boston$lstat), predict(fit, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "purple")
summary(fit)
```

Similar plot with different library:

```{r}
#Fit a polynomial model of degree 3 by regression qualtile library - rq
fit <- rq(medv ~ poly(lstat, 3), data = Boston, tau = 0.5)
plot(Boston$lstat, Boston$medv, 
     main = "Median Property Value vs. Lower Status Population (DEGREE 3)",
     xlab = "Lower Status Population",
     ylab = "Median Property Value")
lines(sort(Boston$lstat), 
      predict(fit, newdata = data.frame(lstat = sort(Boston$lstat))), 
      col = "purple")
#summary(fit)
```

Inference:

As we can see from the above plots of degree of polynomial = 1 and degree of polynomial = 3, we can see a better fit in degree of polynomial = 3 rather than degree of polynomial = 1 .

This is because degree of polynomial = 3 model adjusts more to the current data

For degree of polynomial = 1 we can see high bias whereas for degree of polynomial = 3 there is much more optimal fit.

**2b Repeat with each robust method covered in the lecture notes/slides**.

### Robust methods for polynomial modeling

#### RQ

```{r}
fit_rq <- rq(medv ~ poly(lstat, 3), data = Boston, tau = 0.5)
plot(Boston$lstat, Boston$medv, 
     main = "Median Property Value vs. Lower Status Population(RQ)",
     xlab = "Lower Status Population", 
     ylab = "Median Property Value")
lines(sort(Boston$lstat), 
      predict(fit_rq, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "purple", lwd = 2)

```

#### Huber's M-estimation:

```{r}
m.huber = rlm(Boston$medv ~ poly(Boston$lstat, 3), psi = psi.huber)
summary(m.huber)
plot(Boston$lstat, Boston$medv, 
     main = "Median Property Value vs. Lower Status Population(Huber)", 
     xlab = "Lower Status Population", 
     ylab = "Median Property Value")
lines(sort(Boston$lstat),
      predict(m.huber, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "purple", lwd = 2)
```

#### Hampel's M-estimation:

```{r}
m.hampel = rlm(Boston$medv ~ poly(Boston$lstat, 3), psi = psi.hampel)
summary(m.hampel)
plot(Boston$lstat, Boston$medv, 
     main = "Median Property Value vs. Lower Status Population(Hampel)",
     xlab = "Lower Status Population", 
     ylab = "Median Property Value")
lines(sort(Boston$lstat),
      predict(m.hampel, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "purple", lwd = 2)
```

#### Tukey's M-estimation:

```{r}
m.tukey = rlm(Boston$medv ~ poly(Boston$lstat, 3), psi = psi.bisquare)
summary(m.tukey)
plot(Boston$lstat, Boston$medv,
     main ="Median Property Value vs. Lower Status Population(Tukey)", 
     xlab = "Lower Status Population", 
     ylab = "Median Property Value")
lines(sort(Boston$lstat), 
      predict(m.tukey, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "purple", lwd = 2)
```

#### High breakdown point methods - Least Median of Squares (LMS)

```{r}
# high breakdown point methods
fit.lms = lmsreg(medv ~ lstat, data = Boston)
summary(fit.lms)
plot(Boston$lstat, Boston$medv, 
     main =  "Median Property Value vs. Lower Status Population(LMS)", 
     xlab = "Lower Status Population", ylab = "Median Property Value")
lines(sort(Boston$lstat), 
      predict(fit.lms,
      newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "purple", lwd = 2)
```

#### High breakdown point methods - Least Trimmed Squares (LTS)

```{r}
fit.lts = ltsreg(medv ~ lstat, data = Boston)
summary(fit.lts)
plot(Boston$lstat, Boston$medv,
     main =  "Median Property Value vs. Lower Status Population(LTS)", 
     xlab = "Lower Status Population", ylab = "Median Property Value")
lines(sort(Boston$lstat), 
      predict(fit.lts,
      newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "purple", lwd = 2)
```

#### **2c Produce a scatterplot and overlay all these fits with different colors and a legend.**

```{r}

plot(Boston$lstat, Boston$medv, 
     main = "Scatterplot and Overlay for robust methods", 
     xlab = "Lower Status Population", ylab = "Median Property Value")
lines(sort(Boston$lstat), predict(fit_rq, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "purple", lwd = 2)
lines(sort(Boston$lstat), predict(m.hampel, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "darkgrey", lwd = 2)
lines(sort(Boston$lstat), predict(m.huber, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "green", lwd = 2)
lines(sort(Boston$lstat), predict(m.tukey, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "orange", lwd = 2)
lines(sort(Boston$lstat), predict(fit.lms, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "red", lwd = 2)
lines(sort(Boston$lstat), predict(fit.lts, newdata = data.frame(lstat = sort(Boston$lstat))),
      col = "blue", lwd = 2)
legend("topright", inset = 0.02, 
       title = "Legend", c("RQ", "Hampel","Huber","Tukey","LMS","LTS"),
       fill = c("purple","darkgrey","green","orange","red","blue"), 
       horiz = FALSE, cex = 0.7)

```

Comparative Analysis:
- The overlay shows how each robust method deals with the data's variability and outliers.
- This comparative visualization will aid in selecting the most appropriate robust method for our predictive modeling.
